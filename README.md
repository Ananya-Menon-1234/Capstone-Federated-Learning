# Capstone-Federated-Learning

Link to datasets-Man in the Middle dataset and Active Wiretap dataset
https://www.kaggle.com/datasets/ymirsky/network-attack-dataset-kitsune

Methodology:

The methodology begins by subjecting the datasets to the following supervised machine learning, ensemble, and deep learning models: Random Forest, Logistic Regression, XGBoost, Light Gradient Boosting Machine, and the Deep Learning Models: Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and the evaluation metrics: Accuracy, Precision, Recall, and F1 Score. Traditional Machine Learning models are further optimized by introducing Federated Learning methodology, which not only provides a secure IoT attack detection framework but also involves sharing model updates among clients and the server rather than the raw data and ensures model training while maintaining data privacy and security; it is especially useful in scenarios where centralized training is not feasible. The objective is to see if federated learning offers a competitive performance to a traditional centralized training scenario.The dataset is partitioned among clients. The server only contains the dataset for testing. The server initially sends a copy of the global model to all clients, which contains the dataset for training. There are 100 clients in total. Federated learning undergoes a certain number of training rounds, in this case, we have 10 training rounds. For every training round, 10 clients are chosen at random for every round of training-however, it is ensured that no clients are repeated and all 100 clients are utilized. The random selection of 10 clients out of a total of 100 for each adversarial training round adds an element of diversity and randomness to the training process. This is to ensure that the global model is exposed to a variety of data distributions and characteristics from different clients. The clients train on their local train dataset for one epoch, and send model updates based on parameters- in this case, our model parameters are learning rate(initially set to 0.01), beta 1, and beta 2 for our Adam optimizer (being 0.9 and 0.999) respectively. Adversarial samples are generated using the Fast Gradient Sign method which adds a small perturbation to the original train samples in the direction of the gradient of the loss with respect to the input train samples. The model is trained by the clients both on clean and adversarial samples, increasing its robustness to attacks and leading to an improvement in attack classification. The train function is used by every client to locally train on one epoch and on each epoch for every training round, it is trained on both clean and adversarial samples. The loss function or criterion used is cross-entropy Loss. The updates which are computed by taking a difference of the server's deployed global model parameters and the clients' local model parameters are sent to the server from all 10 clients and using the FedAvg strategy, the server performs an aggregation on the client model parameters and updates the global model. After updating, 25 clients are chosen at random to test the updated global model on their local validation datasets. The aggregated accuracy, precision, recall, and f1 score from all 25 validation datasets is sent to the server. The server uses this information to monitor the performance of the global model and to see if the global model can be improved for the next training round. Finally, after all 10 fit training rounds, the server uses the updated model to test or evaluate the testing set that is present at the serverâ€™s end. The overall accuracy, precision, recall, and f1 score after evaluating the test set is returned.
